= RHQ Inventory

:toc:

== Architecture

RHQ.next inventory is going to be a radical departure from the simplistic
"forest" view supported in the RHQ 4.x (forest meaning multiple trees,
where each tree represents an inventory on a single platform).

The first premise is to allow the inventory to be a directed *cyclic* 
graph. We should not disallow cycles because they can and do happen in the 
real life where "things" are mutually (indirectly) dependent.

A second important premise is that data is fed to RHQ from multiple 
sources, be it the traditional RHQ agent (that will probably at least 
conceptually survive), in-app "feedlets" that send data to RHQ in a very 
simple format similar to that of graphite for example (in fact, we should 
probably support multiple such formats) or even simple bash scripts 
collecting a bunch of data and "curling" it up to the rhq endpoint in some 
format (graphite?). Also we should support harvesting information out of 
3rd party monitoring/management products like collectd, openLMI or other.

These two premises have large consequences that are discussed below.

The reasons for the departure are the following facts that we found during 
the prior versions of RHQ:

* Tree is not enough to capture all the various relationships between 
resources. While it is good to capture the "containment" of services within 
processes, it cannot capture relationships between independent processes or 
machines well.

* While the agents are necessary for some forms of monitoring/config, they 
also have a number of drawbacks, so they should not be the sole way of 
getting data into RHQ:

** _heavyweight_ - the larger number of things "monitored" by the agent, 
the more resources the agent consumes (and we have a relatively bad track 
record for memory and CPU consumption).

** _poll model_ - polling the managed resources generates load on them 
(but any type of monitoring will create that, of course). Scheduling is 
critical, because it can cause resource consumption spikes both in the 
managed resource and in the agent itself.

** _security_ - in order for the agent to perform management tasks on many 
kinds of resources, it needs quite broad privileges. It is also difficult 
to run processes under specific accounts to limit their privileges.

=== Basic Picture

The basic idea is not that different from the RHQ architecture as it stands 
today.

[ditaa, basic-picture]
....

+---------------------------------+
| Machine/VM/Container            |
|                                 |
|+-------------+ +------------+   |
||Application  | |Application |   |
||             | |            |   |
||Data         | |Data        |   |
|+-+-----------+ +--+---------+   |
|  |                |             |      +----------------+
|  |                \------------------->|RHQ endpoint(s) |
|  \------------------------------------>|                |
|+-------------+                  |      |(might or might |
|| Agent       |                  |      |not be a server)|
||             +------------------------>|                |
|| Plugins     |                  |      +----------------+
|+-------------+                  |           ^^   ^
|  ^   ^                          |           ||   |
|  |   |                          |           ||   |
|  |   \-----------\              |           ||   |
|  |               |              |           ||   |
|  v               v              |           ||   |
|+-------------+ +-------------+  |           ||   |
||Application  | |Application  |  |           ||   |discovery
|+-------------+ +-------------+  |           ||   |
|                                 |           ||   |
|   /-----------------------------------------/|   |
|   |            /-----------------------------/   |    
|   |            |                |                |    
|+--+-----+  +---+---+            |                |    
||Collectd|  |OpenLMI|            |           +----+-----+
|+--------+  +-------+            |           |Kubernetes|
+---------------------------------+           +----------+

....

The main differences are:

* Apps can send data directly "out".
* "Out" is not necessarily an RHQ server but merely some kind of endpoint.
* RHQ can consume/receive data from other monitoring/management solutions.

This leaves us with the following questions:

* How to identify the data that come from the applications (we want to 
impose as little as possible on the applications in terms of the format or 
RHQ-specific identification).

* How to identify the agents?

* Do the agents themselves form some kind of hierarchy?

=== Versioning

TODO

=== Discovery Workflow

We ought to consume management data from other systems like OpenLMI or 
Kubernetes, collectd, etc. Some of these systems do the discovery of their 
own and it would be a waste of resources to not reuse the discovery results 
made by the others.

In the below diagram, one can see that using stuff from other systems like 
Kubernetes would require a sort of proxy that would translate the inventory 
as known to the other system to discovery results inside RHQ.

A hierarchy resolver in the sequence diagram below is not a single 
component but rather a number of scripts/server plugins/whatever that uses 
RHQ server's API to examine the "raw" discovery results (i.e. results 
directly reported by the agents) and "munges" them to discover additional 
relationships between them before they enter the inventory. This step is 
necessary prior to import because it might influence the identity of the 
data as computed by RHQ. This is of course prone to "race conditions" of 
sorts where a user might import stuff before the "true" identity of the 
agent is established. I am not 100% sure but this should not pose any 
particular problem though because the identity of the agent can change as 
long as we keep a "link" to its previous identity using the versioning 
mechanism outlined elsewhere.

Hierarchy resolvers serve two purposes:

. Establish a "full identity" of data coming from agents that are 
unreasonably dumb (i.e. ones unable to establish full path to the 
data on the machine).

. Convert the IP address / hostname of the incoming discovery results into 
something more permanent if possible (Kubernetes pod, actual agent ID as 
some logical name - resilient against IP changes in cloud env).

The following example illustrates the workflow of the agent hierarchy 
resolution:

[plantuml, discovery-workflow-1]
....

autonumber 1

box "RHQ"
    participant "Discovery Queue" as DQ
    participant "Hierarchy Resolver" as HR
    participant "Kubernetes Proxy" as KP
endbox

participant "Platform Agent 1" as A1
participant "Platform Agent 2" as A2
participant "In-App Agent" as IA
participant "Kubernetes" as K

A1 -> DQ : capabilities, IP = 1.2.3.1
activate DQ
note left : identifying, discovery, resources, ...
DQ --> A1 : ID = "a1"
deactivate DQ
...
A1 -> DQ : discovery results, ID = a1, IP = 1.2.3.1
activate DQ
DQ --> A1 : ACK
DQ -> HR : notify
HR -> DQ : analyze
deactivate DQ

....

The discovery results of `Platform Agent 1` would look like:

[plantuml, discovery-results-platform-agent1]
....

object "A1/Wildfly" as pa1Wfly {
    ...
}

object "A1/Wildfly/Datasource" as pa1DS {
    ...
}

object "A1/Wildfly/my-app.war" as pa1War {
    ...
}

pa1DS -> pa1Wfly : childOf >
pa1War -> pa1Wfly  : childOf >

....

At this point in time there are no other discovery results so this is also 
the contents of the discovery queue.

[plantuml, discovery-queue-1]
....

package "Platform Agent 1, ID = a1, IP = 1.2.3.1" {
    object "A1/Wildfly" as pa1Wfly {
        ...
    }

    object "A1/Wildfly/Datasource" as pa1DS {
        ...
    }

    object "A1/Wildfly/my-app.war" as pa1War {
        ...
    }

    pa1DS -> pa1Wfly : childOf >
    pa1War -> pa1Wfly  : childOf >
}
....

Next, the second platform agent sends its discovery results in.

[plantuml, discovery-workflow-2]
....

autonumber 7

box "RHQ"
    participant "Discovery Queue" as DQ
    participant "Hierarchy Resolver" as HR
    participant "Kubernetes Proxy" as KP
endbox

participant "Platform Agent 1" as A1
participant "Platform Agent 2" as A2
participant "In-App Agent" as IA
participant "Kubernetes" as K

A2 -> DQ : capabilities, IP = 1.2.3.2
activate DQ
note left : identifying, discovery, resources, ...
DQ --> A2 : ID = "a2"
deactivate DQ
...
A2 -> DQ : discovery results, ID = a2, IP = 1.2.3.3
activate DQ
DQ --> A2 : ACK
DQ -> HR : notify
HR -> DQ : analyze

....

The discovery results of `Platform Agent 2` would look like:

[plantuml, discovery-results-platform-agent2]
....

object "A2/Postgres" as pa2Postgres {
    ...
}

object "A2/Postgres/Table 'kachna'" as pa2Kachna {
    ...
}

pa2Kachna -> pa2Postgres : childOf >

....

And the discovery queue would end up looking like this. Note that because 
the agent supports identity, the change of its IP between its "announce" 
and sending of the actual discovery results didn't confuse things.

[plantuml, discovery-queue-2]
....

package "Platform Agent 1, ID = a1, IP = 1.2.3.1" {
    object "A1/Wildfly" as pa1Wfly {
        ...
    }

    object "A1/Wildfly/Datasource" as pa1DS {
        ...
    }

    object "A1/Wildfly/my-app.war" as pa1War {
        ...
    }

    pa1DS -> pa1Wfly : childOf >
    pa1War -> pa1Wfly  : childOf >
}

package "Platform Agent 2, ID = a2, IP = 1.2.3.3" {
    object "A2/Postgres" as pa2Postgres {
        ...
    }

    object "A2/Postgres/Table 'kachna'" as pa2Kachna {
        ...
    }

    pa2Kachna -> pa2Postgres : childOf >
}
....

Next, the `In-App Agent` reports. This is a dumb agent that doesn't know a 
thing about its environment, it just wants to send `name=value` pairs.

[plantuml, discovery-workflow-3]
....

autonumber 13

box "RHQ"
    participant "Discovery Queue" as DQ
    participant "Hierarchy Resolver" as HR
    participant "Kubernetes Proxy" as KP
endbox

participant "Platform Agent 1" as A1
participant "Platform Agent 2" as A2
participant "In-App Agent" as IA
participant "Kubernetes" as K

IA -> DQ : discovery results, IP = 1.2.3.4
activate DQ
note left : huh, no capabilities, defaults to "monitor"
DQ --> IA : ACK
DQ -> HR : notify
HR -> DQ : analyze
deactivate DQ

....

RHQ doesn't have any other information but the IP address to identify this 
agent (it doesn't support identifying itself), so the discovery queue will 
look as follows momentarily:

[plantuml, discovery-queue-3]
....

package "Platform Agent 1, ID = a1, IP = 1.2.3.1" {
    object "A1/Wildfly" as pa1Wfly {
        ...
    }

    object "A1/Wildfly/Datasource" as pa1DS {
        ...
    }

    object "A1/Wildfly/my-app.war" as pa1War {
        ...
    }

    pa1DS -> pa1Wfly : childOf >
    pa1War -> pa1Wfly  : childOf >
}

package "Platform Agent 2, ID = a2, IP = 1.2.3.3" {
    object "A2/Postgres" as pa2Postgres {
        ...
    }

    object "A2/Postgres/Table 'kachna'" as pa2Kachna {
        ...
    }

    pa2Kachna -> pa2Postgres : childOf >
}

package "In-App Agent, ID = ?, IP = 1.2.3.4" {
    object "Button" as iaBtn {
        ...
    }

    object "Page" as iaPage {
        ...
    }
}

....

Now, we receive from the `Kubernetes Proxy` agent that reports `Kubernetes` 
structure:

[plantuml, discovery-workflow-4]
....

autonumber 17

box "RHQ"
    participant "Discovery Queue" as DQ
    participant "Hierarchy Resolver" as HR
    participant "Kubernetes Proxy" as KP
endbox

participant "Platform Agent 1" as A1
participant "Platform Agent 2" as A2
participant "In-App Agent" as IA
participant "Kubernetes" as K

KP -> DQ : capabilities, IP = 1.2.3.5
activate DQ
note left : identifying, discovery, resources
DQ --> KP : ID = "kp"
deactivate DQ
...
KP -> K : poll
activate KP
KP -> DQ : discovery results, ID = kp, IP = 1.2.3.5
activate DQ
DQ --> KP : ACK
deactivate KP
DQ -> HR : notify
HR -> DQ : analyze
deactivate DQ
....

This agent reports the following discovery results:

[plantuml, discovery-results-kubernetes]
....

object "Service 1" as srvc1 {
    ...
}

object "Pod 1" as pod1 {
    ...
}

object "Pod 2" as pod2 {
    ...
}

object "Pod 1 Container 1" as pod1Cont1 {
    ...
}

object "Pod 1 Container 2" as pod1Cont2 {
    ...
}

object "Pod 2 Container 1" as pod2Cont1 {
    ...
}

pod1 -> srvc1 : childOf >
pod2 -> srvc1 : childOf >

pod1Cont1 -- pod1 : childOf >
pod1Cont2 -- pod1 : childOf >
pod2Cont1 -- pod2 : childOf >

....

Now, the hierarchy resolver kicks in and realizes, based on comparing the 
data in the discovery results of the kubernetes proxy and the agents' IP 
addresses that the IP address of `In-App Agent` corresponds to an IP 
address of pod `Pod 1`. It is therefore able to transform the discovery 
queue to look like this:

[plantuml, discovery-queue-4]
....

package "Platform Agent 1, ID = a1, IP = 1.2.3.1" {
    object "A1/Wildfly" as pa1Wfly {
        ...
    }

    object "A1/Wildfly/Datasource" as pa1DS {
        ...
    }

    object "A1/Wildfly/my-app.war" as pa1War {
        ...
    }

    pa1DS -> pa1Wfly : childOf >
    pa1War -> pa1Wfly  : childOf >
}

package "Platform Agent 2, ID = a2, IP = 1.2.3.3" {
    object "A2/Postgres" as pa2Postgres {
        ...
    }

    object "A2/Postgres/Table 'kachna'" as pa2Kachna {
        ...
    }

    pa2Kachna -> pa2Postgres : childOf >
}

package "In-App Agent, ID = ?, IP = 1.2.3.4" {
    object "Button" as iaBtn {
        ...
    }

    object "Page" as iaPage {
        ...
    }
}

package "Kubernetes Proxy, ID = kp, IP = 1.2.3.5" {
    object "Service 1" as srvc1 {
        ...
    }

    object "Pod 1" as pod1 {
        ...
    }

    object "Pod 2" as pod2 {
        ...
    }

    object "Pod 1 Container 1" as pod1Cont1 {
        ...
    }

    object "Pod 1 Container 2" as pod1Cont2 {
        ...
    }

    object "Pod 2 Container 1" as pod2Cont1 {
        ...
    }

    pod1 -> srvc1 : childOf >
    pod2 -> srvc1 : childOf >

    pod1Cont1 -- pod1 : childOf >
    pod1Cont2 -- pod1 : childOf >
    pod2Cont1 -- pod2 : childOf >
}

iaBtn -- pod1 : childOf >
iaPage -- pod1 : childOf >

....

Notice that it was not able to realize what container the application 
managed by the `In-App Agent` runs in, because that information is not 
available in kubernetes (containers in a pod share IP address and (parts 
of) filesystem), but it was able to figure out that the resources are part 
of the Pod1 resource based on the IP info from kubernetes. The user could 
have the possibility to reorganize the the resource hierarchy even further 
by specifying that the 2 resources lived under `Pod1 Container2` resource 
if they so wished. The ID of the agent remains unassigned though because 
there is no way of knowing what agent is reporting if there were more of 
them inside `Pod 1`.

When the IP of the `Pod 1` changes again, we receive data from the `In-App 
Agent` from that different IP and because it doesn't know how to identify 
itself, it'll end up "looking" like a new agent momentarily. As soon as the 
kubernetes part of the hierarchy resolver sees this, it checks that IP 
again, realizes that it now is the IP of `Pod 1` and will assign the `pod1` 
ID to that agent and merge the "newly discovered" resources with the 
already existing ones.

=== Agent Identity

The previous chapter, <<Discovery Workflow>>, touched on the agent identity 
already.

Basically, the agent can either be capable of remembering its RHQ-assigned 
ID or, when it's not, the agent ID might be deduced on the server-side by 
hierarchy resolvers.

If that fails, too, there should be a fallback option to manually map a new 
agent in the discovery queue to some other existing agent using our APIs. 
The discovered resources would be merged with the existing ones in some way 
or fashion.

=== Data Identity

Currently in RHQ, resources are the primary entities with identity. Identity 
of other incoming data like measurements, configurations, etc. are derived 
from the identity of the resource.

As I outlined in the
https://docs.jboss.org/author/display/RHQ/Proposal+-+Poly-agent+RHQ[Poly-
agent proposal], I think that for us to enable agent-less monitoring (i.e. 
apps/scripts directly sending data), we need to be able to uniquely identify 
individual measurements, configurations, etc. This is to a) not impose too 
much responsibility for RHQ-specific identification on the senders and b) be 
able to compose ad-hoc resources in the server from data coming from agents.

Technically we would probably still use a "dummy" resource even for agents 
that do not understand the concept of multiple resources per agent so that 
resourceAddress + datumID becomes a unique identifier of a piece of data. 
The resource address would probably be a combination of agentID + 
resourceAncestry where ancestry is just a "path" of agent-local resource 
identifiers.

This arrangement would also enable us to consume data from other systems 
where the agentID would identify the other system and resourceAncestry and 
datumID would describe the "path" to the piece of data in that system. Maybe 
we could even encode the whole of that as an URI to not require multiple 
fields to identify the data.

`agent-type://agentName/resourceAddr/datumID`

* `_agent-type_` - identifies the type of the agent (somewhat obviously). 
This can be used to keep track of what 3rd party system was used to obtain 
the information. Possible values would be `rhq`, `collectd`, `cadvisor`, 
etc.
+
Note that RHQ will usually not be able to determine this value but rather 
would have to be told (possibly by URL path it receives the data on) what 
the agent type is. This is because the type of the agent can be different 
than the format we received the data in (i.e. we can receive a datapoint as 
a graphite triplet, a json object, ... but that doesn't tell us what system 
this value comes from).
+
We need to distinguish different agent types because the `agentName` 
format/value will depend on it. It is possible that 2 types of agents have 
the same logical "name".

* `_agentName_` - The (possibly empty) name of the agent as established by 
the hierarchy resolver.
+
For example the collectd resolver will use the host part of the collectd 
identifier to get the host which then can be further analyzed by for 
example the kubernetes resolver to determine the name of the pod which is 
finally used as the agent name.

* `_resourceAddr_` - The possibly empty address of the resource in the 
hierarchy of resources *as understood by the agent*. Note that because of 
the support for composition of custom resources out of arbitrary data on 
the server side, this might be different from the (number of) resource 
addresses as known on the server.

* `_datumID_` - This is the name of the data point. This the exact string 
sent up from the agent and hence it might encode information that is also 
present in other parts of the full data identifier (think about the 
collectd identifiers that always include the host in it). Also note that 
this ID might evolve in time (again think about collectd identifiers when 
the IP of the machine changes). It is up to RHQ to handle the linking of 
the changes (using versioning) using something similar as hierarchy 
resolvers described in the <<Discovery Workflow>> chapter.

Given the description above, the full `agentID` referred to in the previous 
chapters would actually have a form `agent-type://agentName`.

=== Data Types

TODO

=== What Data to Store?

TODO 

=== Storage & Routing of Data

TODO

=== Inventory Navigation and Querying

TODO

== Technology Choices

TODO

=== Graph Databases

TODO

=== RDF

TODO

=== S-RAMP

TODO

=== Kubernetes

TODO

== APIs

TODO

=== Client API

TODO

=== Agent API

TODO
